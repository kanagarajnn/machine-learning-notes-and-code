# Implementing Softmax in a Neural Network

## Overview
- Softmax regression can be incorporated into the **output layer** of a neural network for **multiclass classification**.
- Instead of predicting just two classes (binary classification), Softmax allows classification into **multiple categories**.
- **Real-world applications**:
  - Handwritten digit recognition (0-9)
  - Object classification in images
  - Multi-disease medical diagnosis

---

## How Softmax Works in a Neural Network
### **1. Expanding from Binary to Multiclass**
- Previously, a neural network for **binary classification** had:
  - A **single output unit** with a **sigmoid activation**.
- For **multiclass classification (e.g., digits 0-9)**:
  - The **output layer must have 10 units**.
  - Each unit corresponds to a possible class.
  - The **Softmax activation function** is applied to produce probabilities.

### **2. Forward Propagation with Softmax**
- Given an input `X`, activations from earlier layers are computed normally.
- At the **output layer**, we compute `Z1` to `Z10`:
  ```
  Z1 = W1 * A2 + B1
  Z2 = W2 * A2 + B2
  ...
  Z10 = W10 * A2 + B10
  ```
- These values are transformed using the **Softmax function**:
  ```
  A1 = exp(Z1) / (exp(Z1) + ... + exp(Z10))
  A2 = exp(Z2) / (exp(Z1) + ... + exp(Z10))
  ...
  A10 = exp(Z10) / (exp(Z1) + ... + exp(Z10))
  ```
- The output probabilities sum to **1**, ensuring a proper probability distribution.

**Example:**
- If the input image is **digit '3'**, the model may output:
  ```
  P(y=3 | X) = 0.85 (highest probability)
  ```
  meaning the model is **85% confident** in its prediction.

---

## Unique Properties of Softmax Activation
- Unlike **ReLU** or **sigmoid**, where activations depend only on their respective `Z` values:
  - **Softmax activations depend on all `Z` values**.
  - Each probability is influenced by every other output unit.
- This ensures a **relative probability** distribution rather than absolute values.

---

## Implementing Softmax Neural Network in TensorFlow
### **1. Defining the Model**
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential([
    Dense(25, activation='relu'),  # Hidden Layer 1
    Dense(15, activation='relu'),  # Hidden Layer 2
    Dense(10, activation='softmax')  # Output Layer (10 classes)
])
```

### **2. Compiling the Model**
- TensorFlow uses **Sparse Categorical Crossentropy** for Softmax-based classification:
```python
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```
- **Why `sparse_categorical_crossentropy`?**
  - `sparse` means labels are **single integers** (`0-9` instead of one-hot vectors).
  - Efficient and simplifies model training.

### **3. Training the Model**
```python
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))
```
- The model is trained for **10 epochs** on labeled training data.
- **Accuracy metric** is used to track performance.

---

## Key Takeaways
| Feature | Benefit |
|---------|---------|
| Softmax Output | Converts scores into probabilities |
| Multiclass Support | Handles **more than two** categories |
| Probability Sum | Ensures valid probability distribution |
| TensorFlow Optimization | `sparse_categorical_crossentropy` for efficiency |

---

## Conclusion
- Adding a **Softmax layer** to a neural network enables **multiclass classification**.
- Softmax transforms raw scores into **interpretable probabilities**.
- **Real-World Impact**: Used in AI-powered applications like **Google Photosâ€™ object tagging, voice assistants, and medical imaging systems**.

By implementing Softmax in neural networks, you can build **robust classifiers** for complex real-world problems!




---
## Next Section
- ### [Classification with Multiple Outputs](Multiple_Outputs.md)
